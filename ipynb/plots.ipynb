{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plots.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adelC/3Dgan/blob/adel%2Fpgan/ipynb/plots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFmpU3sbdr2Z"
      },
      "source": [
        "# Loading Google Drive and mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVsG4Ox9Zc_0",
        "outputId": "53e0c52c-895c-43b2-d8f3-57dfe1fcd84c"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEKLIUMZTS1z",
        "outputId": "cd568008-ef1e-44a4-9281-7f106dd385a5"
      },
      "source": [
        "%tensorflow_version 1.14"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "1Wlaguq5r0vO",
        "outputId": "27a3cbfc-3326-4063-8e9d-7d6313e36f59"
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (51.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.2\n",
            "    Uninstalling tensorflow-1.15.2:\n",
            "      Successfully uninstalled tensorflow-1.15.2\n",
            "Successfully installed tensorboard-2.4.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2I_IUEdXqZm",
        "outputId": "6cff4ee5-b712-4b1c-b25a-7ecb3adc4c37"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "yLDHHVzLTaMl",
        "outputId": "d8a80da3-89d3-4455-a832-503094c6ff91"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-53968207222e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_applications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_applications'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xESYzMEshWlQ",
        "outputId": "212d432a-6c58-4a96-a494-60dc941369a1"
      },
      "source": [
        "!pip install horovod"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting horovod\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/93/503830d0de5c7390d65c94ffa60db3719826f3deeb0d055fcfa35b4d7927/horovod-0.21.1.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from horovod) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from horovod) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from horovod) (3.13)\n",
            "Requirement already satisfied: cffi>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from horovod) (1.14.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from horovod) (0.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.0->horovod) (2.20)\n",
            "Building wheels for collected packages: horovod\n",
            "  Building wheel for horovod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for horovod: filename=horovod-0.21.1-cp36-cp36m-linux_x86_64.whl size=20544896 sha256=f636f15e870b15de2d3d8fb6bf234d8c810949c4fe0796724bc01ea5f7497b82\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/91/85/8e868c735394d482d578639f2dc392ae7f69e11a58a95d6108\n",
            "Successfully built horovod\n",
            "Installing collected packages: horovod\n",
            "Successfully installed horovod-0.21.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7VCQigqdoOA"
      },
      "source": [
        "## Setting ROOT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0MDcSPdbjbj"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/tools/root/\")\n",
        "sys.path.append(\"/content/drive/MyDrive/tools/root/bin/\")\n",
        "sys.path.append(\"/content/drive/MyDrive/tools/root/include/\")\n",
        "sys.path.append(\"/content/drive/MyDrive/tools/root/lib/\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxxTcYDGdd5y"
      },
      "source": [
        "import ROOT"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5BkRdnYT20b",
        "outputId": "512e5f65-fcf9-47f6-fd7a-94fc9b9ee9f9"
      },
      "source": [
        "%cd /content/drive/MyDrive/code/cern3/3Dgan/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/code/cern3/3Dgan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "F5LfG3A4U1gs",
        "outputId": "cee3c2ce-1e0d-4ebb-bf2c-bb44aa156d01"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from os import path\n",
        "import argparse\n",
        "import h5py\n",
        "import ROOT\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import keras.backend as K\n",
        "import horovod.tensorflow as hvd\n",
        "from scripts.GANutils import GetData, GetAngleData, GetDataFiles, generate, generate2,  safe_mkdir\n",
        "from scripts.hdf_to_numpy import resize, restore_pic\n",
        "import scripts.ROOTutils as my\n",
        "from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "import time\n",
        "import importlib\n",
        "import glob\n",
        "sys.path.insert(0,'../')\n",
        "try:\n",
        "    import setGPU #if Caltech\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_parser():\n",
        "   # To parse the input parameters to the script\n",
        "   parser = argparse.ArgumentParser()\n",
        "   parser.add_argument('--gweights', type=str, nargs='+', default=['../weights/3dgan_weights_gan_training_epsilon_2_500GeV/params_generator_epoch_021.hdf5'], help=\"Complete PATH to the trained weights to test with hdf5 extension\")\n",
        "   parser.add_argument('--ang', default=1, type=int, help=\"If using angle vefsion\")\n",
        "   parser.add_argument('--pgan', type=int, default=1, help='If using PGAN')\n",
        "   parser.add_argument('--particle', default='Ele', type=str, help=\"particle type\")\n",
        "   parser.add_argument('--labels', type=str, nargs='+', default=[''], help=\"labels for different weights\")\n",
        "   parser.add_argument('--xscales',  type=float, nargs='+', help=\"scaling factor for cell energies\")\n",
        "   parser.add_argument('--datapath', default='full2', help='Data to check the output obtained')\n",
        "   parser.add_argument('--outdir', default= 'results/short_analysis', help='Complete PATH to save the output plot')\n",
        "   parser.add_argument('--numevents', action='store', type=int, default=10000, help='Max limit for events used for validation')\n",
        "   parser.add_argument('--latent', action='store', type=int, help='size of latent space to sample')\n",
        "   parser.add_argument('--dformat', action='store', type=str, default='channels_last', help='keras image format')\n",
        "   parser.add_argument('--error', type=int, default=0, help='add relative errors to plots')\n",
        "   parser.add_argument('--stest', type=int, default=0, help='add ktest to plots')\n",
        "   parser.add_argument('--norm', type=int, default=1, help='normalize shower shapes')\n",
        "   parser.add_argument('--ifC', type=int, default=0, help='generate .C files')\n",
        "   parser.add_argument('--leg', type=int, default=1, help='draw legend')\n",
        "   parser.add_argument('--grid', type=int, default=0, help='draw grid')\n",
        "   return parser\n",
        "\n",
        "\n",
        "def get_session():\n",
        "    gopts = tf.GraphOptions(place_pruned_graph=True)\n",
        "    config = tf.ConfigProto(graph_options=gopts, allow_soft_placement=True)\n",
        "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
        "    sess = tf.Session(config=config)\n",
        "    return sess\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "   #parser = get_parser()\n",
        "   #args = parser.parse_args()\n",
        "   #gweights = args.gweights if isinstance(args.gweights, list) else [args.gweights]\",\n",
        "   #labels = args.labels if isinstance(args.labels, list) else [args.labels]\",\n",
        "   ang = 1 #args.ang\",\n",
        "   pgan = 1 #args.pgan\",\n",
        "   particle = \"Ele\" #args.particle\",\n",
        "   latent = 256 # args.latent\\n\",\n",
        "   datapath = \"/content/drive/MyDrive/dataset/cern/Ele_VarAngleMeas*.h5\" #args.datapath\",\n",
        "   outdir = \"output\" #args.outdir\",\n",
        "   numevents= 10000 #args.numevents\\n\",\n",
        "   dformat= \"channels_last\" #args.dformat\",\n",
        "   error = 0 #args.error\",\n",
        "   stest = 0 #args.stest\",\n",
        "   norm = 1 #args.norm\",\n",
        "   C = 0 #args.ifC\",\n",
        "   leg = 1 #args.leg\",\n",
        "   grid = 0 #args.grid\",\n",
        "   xscales = 1.\n",
        "\n",
        "   Files =sorted( glob.glob(datapath))\n",
        "   print (\"Found {} files. \".format(len(Files)))\n",
        "\n",
        "   dscale = 50.\n",
        "   if not latent:\n",
        "       latent = 256\n",
        "   if not xscales:\n",
        "       xscales = [1] * len(gweights)\n",
        "   xpower = 0.85\n",
        "   \n",
        "   datafiles = GetDataFiles(datapath, particle, 1)\n",
        "   print(\"@@@@@ : \", len(datafiles))\n",
        "   data = datafiles[-1] # use the last file for the plots\n",
        "   X, Y, angle, f = GetAngleData(data, angtype='theta', num_events=numevents)\n",
        "\n",
        "   X = np.squeeze(X)/dscale # convert data to GeV\n",
        "   print(\"X reshape at begining \", X.shape)\n",
        "   # X should not be resized but the generated dataset, this is temperary\n",
        "   #X = resize(X,64)\n",
        "   #X = np.moveaxis(X, 1,3)\n",
        "   print(\"X reshape at begining \", X.shape)\n",
        "   #get shape\n",
        "   x = X.shape[1]\n",
        "   y = X.shape[2]\n",
        "   z = X.shape[3]\n",
        "\n",
        "   #select events with significant energy deposition\n",
        "   xsum = np.sum(X, axis=(1, 2, 3))\n",
        "   indexes = np.where(xsum > (0.2))\n",
        "   X=X[indexes]\n",
        "   Y = Y[indexes]\n",
        "   angle = angle[indexes]\n",
        "\n",
        "   num_events = X.shape[0] # number of events can be reduced due to selection\n",
        "\n",
        "   images =[]\n",
        "   gm=importlib.import_module(f'networks.pgan.generator').generator\n",
        "\n",
        "   sess = get_session()\n",
        "   #saver = tf.train.Saver()\n",
        "   print('******** ROOT DEBUG *******', f)\n",
        "   saver = tf.compat.v1.train.import_meta_graph('/content/drive/MyDrive/code/cern3/model/model_5.meta')\n",
        "   #saver = tf.train.import_meta_graph('/home/achaibi/scratch/applications/cern/runs8/runs/pgan/2020-11-10_09:57:44/model_4.meta')\n",
        "   #saver = tf.train.import_meta_graph( os.path.join(gweights,'model_4_ckpt_4160000.meta'))\n",
        "   #saver.restore(sess, os.path.join(args.continue_path)\n",
        "   saver.restore(sess, tf.train.latest_checkpoint(\"/content/drive/MyDrive/code/cern3/model/\"))\n",
        "   angle = angle\n",
        "   generator = importlib.import_module(f'networks.pgan.generator').generator\n",
        "    \n",
        "   print(\"X SHAPE ####### \", X.shape) \n",
        "   print(\"Y SHAPE ####### \", Y.shape) \n",
        "   X=X[:32]\n",
        "   Y=Y[:32]\n",
        "   angle=angle[:32]\n",
        "   images.append(generate2(generator, 32, [Y/100., angle], latent=latent, concat=2))\n",
        "   images[0] = np.power(images[0], 1./xpower)\n",
        "   \n",
        "   \n",
        "   print(f\"ANGLEPGAN DEBUG ### : images={images}\")\n",
        "   #images = tf.transpose(images, [0, 1, 5, 4, 3, 2])\n",
        "   \n",
        "   numpy_images = []\n",
        "   for i, imgs in enumerate(images):\n",
        "\n",
        "      with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        #test = gsf.eval()\n",
        "        #imgs = tf.squeeze(imgs,[1])\n",
        "        #imgs = tf.transpose(imgs, [0,4,2,3,1])\n",
        "        imgs = sess.run(imgs)\n",
        "        imgs = restore_pic(imgs, 64)\n",
        "        numpy_images.append(imgs)\n",
        "\n",
        "   plotSF(X, numpy_images, Y, labels, out_file=outdir +'/SamplingFraction', session=sess, error=error, stest=stest, ifC=C,grid=grid,leg=leg)\n",
        "   plotshapes(X, numpy_images, x, y, z, Y, out_file=outdir +'/ShowerShapes',labels=labels, log=0, stest=stest, error=error, norm=norm, ifC=C, grid=grid, leg=leg)\n",
        "   plotshapes(X, numpy_images, x, y, z, Y, out_file=outdir +'/ShowerShapes_log',labels=labels, log=1, stest=stest, error=error, norm=norm, ifC=C, grid=grid, leg=leg)\n",
        "   print('The plots are saved in {}'.format(outdir))\n",
        "\n",
        "#Plotting sampling fraction vs. Ep\n",
        "def plotSF(Data, gan_images, Y, labels, out_file, session, error=0, stest=0, ifC=0, grid=0, leg=1, ):\n",
        "   c=ROOT.TCanvas(\"c\" ,\"Sampling Fraction vs. Primary energy\" ,200 ,10 ,700 ,500) #make nice\n",
        "   if grid: c.SetGrid()\n",
        "   color =2\n",
        "   ROOT.gStyle.SetOptStat(0)\n",
        "   Eprof = ROOT.TProfile(\"Eprof\", \"Ratio of Ecal and Ep;Ep;Ecal/Ep\", 100, 0, 500)\n",
        "   dsum = np.sum(Data, axis=(1,2, 3))\n",
        "   print(\"dsum shape :::\", dsum.shape)\n",
        "   dsf = dsum/Y\n",
        "   for j in np.arange(Y.shape[0]):\n",
        "     Eprof.Fill( Y[j], dsf[j])\n",
        "   Eprof.SetTitle(\"Sampling Fraction (cell energy sum / primary particle energy)\")\n",
        "   Eprof.GetXaxis().SetTitle(\"Primary particle energy [GeV]\")\n",
        "   Eprof.GetYaxis().SetTitle(\"Sampling Fraction\")\n",
        "   Eprof.GetYaxis().SetRangeUser(0.01, 0.03)\n",
        "   Eprof.SetLineColor(color)\n",
        "   Eprof.Draw()\n",
        "   if stest:\n",
        "     legend = ROOT.TLegend(0.6, 0.11, 0.89, 0.4)\n",
        "   else:\n",
        "     legend = ROOT.TLegend(0.7, 0.11, 0.89, 0.3)\n",
        "   legend.AddEntry(Eprof, \"Data\", \"l\")\n",
        "   legend.SetBorderSize(0)\n",
        "   Gprof = []\n",
        "   for i, images in enumerate(gan_images):\n",
        "      Gprof.append( ROOT.TProfile(\"Gprof\" +str(i), \"Gprof\" + str(i), 100, 0, 500))\n",
        "      gsum = np.sum(images, axis=(1, 2, 3))\n",
        "      gsf = gsum/Y\n",
        "\n",
        "      for j in range(Y.shape[0]):\n",
        "        Gprof[i].Fill(Y[j], gsf[j])\n",
        "      color = color + 2\n",
        "      Gprof[i].SetLineColor(color)\n",
        "      Gprof[i].Draw('sames')\n",
        "      c.Modified()\n",
        "      sf_error = np.absolute((dsf-gsf)/dsf)\n",
        "      glabel = 'GAN {}'.format(labels[i])\n",
        "      if error:\n",
        "         glabel = glabel + ' MRE={:.4f}'.format(np.mean(sf_error))\n",
        "      legend.AddEntry(Gprof[i], glabel, \"l\")\n",
        "      if stest:\n",
        "         ks = Eprof.KolmogorovTest(Gprof[i], 'WW')\n",
        "         legend.AddEntry(Gprof[i], 'k={:e}'.format(ks), \"l\")\n",
        "      if leg: legend.Draw()\n",
        "      c.Update()\n",
        "   c.Print(out_file+'.pdf')\n",
        "   if ifC:\n",
        "      c.Print(out_file+'.C')\n",
        "\n",
        "# plotting shower shapes\n",
        "def plotshapes(X, generated_images, x, y, z, energy, out_file, labels, log=0, p=[2, 500], norm=0, ifC=0, stest=0, error=0, grid=0, leg=1):\n",
        "   canvas = ROOT.TCanvas(\"canvas\" ,\"\" ,200 ,10 ,700 ,500) #make\n",
        "   canvas.SetTitle('Weighted Histogram for energy deposition along x, y, z axis')\n",
        "   if grid: canvas.SetGrid()\n",
        "   color = 2\n",
        "   canvas.Divide(2,2)\n",
        "   print(\"IMAGE X &&&&&&& :\", X.shape)\n",
        "   # THIS IS TEMPORARY! the generated image should have the same shape as the real images and not the contrary\n",
        "   array1x = np.sum(X, axis=(2,3))\n",
        "   array1y = np.sum(X, axis=(1,3))\n",
        "   array1z = np.sum(X, axis=(1,2))\n",
        "   if stest:\n",
        "     leg = ROOT.TLegend(0.1,0.1,0.9,0.9)\n",
        "   else:\n",
        "     leg = ROOT.TLegend(0.1,0.4,0.9,0.9)\n",
        "   #leg.SetTextSize(0.06)\n",
        "   h1x = ROOT.TH1F('G4x' + str(energy), '', x, 0, x)\n",
        "   h1y = ROOT.TH1F('G4y' + str(energy), '', y, 0, y)\n",
        "   h1z = ROOT.TH1F('G4z' + str(energy), '', z, 0, z)\n",
        "   h1x.Sumw2()\n",
        "   h1y.Sumw2()\n",
        "   h1z.Sumw2()\n",
        "   h1x.SetLineColor(color)\n",
        "   h1y.SetLineColor(color)\n",
        "   h1z.SetLineColor(color)\n",
        "   color+=2\n",
        "   canvas.cd(1)\n",
        "   if log:\n",
        "      ROOT.gPad.SetLogy()\n",
        "   my.fill_hist_wt(h1x, array1x)\n",
        "   if norm: h1x=my.normalize(h1x)\n",
        "   h1x.Draw()\n",
        "   h1x.Draw('sames hist')\n",
        "   h1x.GetXaxis().SetTitle(\"Energy deposition along x axis\")\n",
        "   leg.AddEntry(h1x, 'G4',\"l\")\n",
        "   canvas.cd(2)\n",
        "   if log:\n",
        "      ROOT.gPad.SetLogy()\n",
        "   my.fill_hist_wt(h1y, array1y)\n",
        "   if norm: h1y=my.normalize(h1y)\n",
        "   h1y.Draw()\n",
        "   h1y.Draw('sames hist')\n",
        "   h1y.GetXaxis().SetTitle(\"Energy deposition along y axis\")\n",
        "   canvas.cd(3)\n",
        "   if log:\n",
        "      ROOT.gPad.SetLogy()\n",
        "   my.fill_hist_wt(h1z, array1z)\n",
        "   if norm : h1z=my.normalize(h1z)\n",
        "   h1z.Draw()\n",
        "   h1z.Draw('sames hist')\n",
        "   h1z.GetXaxis().SetTitle(\"Energy deposition along z axis\")\n",
        "   canvas.cd(4)\n",
        "   canvas.Update()\n",
        "   h2xs=[]\n",
        "   h2ys=[]\n",
        "   h2zs=[]\n",
        "   for i, images in enumerate(generated_images):\n",
        "      array2x = np.sum(images, axis=(2,3))\n",
        "      #array2x = tf.reduce_sum(images, axis=(2,3))\n",
        "      array2y = np.sum(images, axis=(1,3))\n",
        "      #array2y = tf.reduce_sum(images, axis=(1,3))\n",
        "      array2z = np.sum(images, axis=(1,2))\n",
        "      #array2z = tf.reduce_sum(images, axis=(1,2))\n",
        "      errorx = np.divide(np.absolute(array1x-array2x), array1x, out=np.zeros_like(array1x), where=array1x!=0)\n",
        "      errory = np.divide(np.absolute(array1y-array2y), array1y, out=np.zeros_like(array1y), where=array1y!=0)\n",
        "      errorz = np.divide(np.absolute(array1z-array2z), array1z, out=np.zeros_like(array1z), where=array1z!=0)\n",
        "\n",
        "      h2xs.append(ROOT.TH1F('GANx' + str(energy)+ labels[i], '', x, 0, x))\n",
        "      h2ys.append(ROOT.TH1F('GANy' + str(energy)+ labels[i], '', y, 0, y))\n",
        "      h2zs.append(ROOT.TH1F('GANz' + str(energy)+ labels[i], '', z, 0, z))\n",
        "      h2x=h2xs[i]\n",
        "      h2y=h2ys[i]\n",
        "      h2z=h2zs[i]\n",
        "      h2x.Sumw2()\n",
        "      h2y.Sumw2()\n",
        "      h2z.Sumw2()\n",
        "\n",
        "      h2x.SetLineColor(color)\n",
        "      h2y.SetLineColor(color)\n",
        "      h2z.SetLineColor(color)\n",
        "      canvas.cd(1)\n",
        "      my.fill_hist_wt(h2x, array2x)\n",
        "      if norm: h2x=my.normalize(h2x)\n",
        "      h2x.Draw('sames')\n",
        "      h2x.Draw('sames hist')\n",
        "      canvas.Update()\n",
        "      #my.stat_pos(h2x)\n",
        "      if stest:\n",
        "         res=np.array\n",
        "         ks= h1x.KolmogorovTest(h2x, 'WW')\n",
        "         glabel = \"GAN {} X axis k= {:e}\".format(labels[i], ks)\n",
        "         leg.AddEntry(h2x, glabel,\"l\")\n",
        "      canvas.Update()\n",
        "      canvas.cd(2)\n",
        "      my.fill_hist_wt(h2y, array2y)\n",
        "      if norm: h2y=my.normalize(h2y)\n",
        "      h2y.Draw('sames')\n",
        "      h2y.Draw('sames hist')\n",
        "      canvas.Update()\n",
        "      #my.stat_pos(h2y)\n",
        "      if stest:\n",
        "         ks= h1y.KolmogorovTest(h2y, 'WW')\n",
        "         glabel = \"GAN {} Y axis k= {:e}\".format(labels[i], ks)\n",
        "         leg.AddEntry(h2y, glabel,\"l\")\n",
        "      canvas.Update()\n",
        "      canvas.cd(3)\n",
        "      my.fill_hist_wt(h2z, array2z)\n",
        "      if norm: h2z=my.normalize(h2z)\n",
        "      h2z.Draw('sames')\n",
        "      h2z.Draw('sames hist')\n",
        "      canvas.Update()\n",
        "      #my.stat_pos(h2z)\n",
        "      canvas.Update()\n",
        "      if stest:\n",
        "         ks= h1z.KolmogorovTest(h2z, 'WW')\n",
        "         glabel = \"GAN {} Z axis k= {:e}\".format(labels[i], ks)\n",
        "         leg.AddEntry(h2z, glabel,\"l\")\n",
        "      canvas.Update()\n",
        "      color+=2\n",
        "   canvas.Update()\n",
        "   canvas.cd(4)\n",
        "   leg.SetHeader(\"Energy deposited along x, y, z axis\", \"C\")\n",
        "\n",
        "   for i, h in enumerate(h2xs):\n",
        "       glabel = 'GAN ' + labels[i]\n",
        "       if error:\n",
        "          tot_error = (np.mean(errorx) + np.mean(errory) + np.mean(errorz))/3.\n",
        "          glabel = glabel + ' MRE {:.4f}'.format(np.mean(errorz))\n",
        "       elif not stest:\n",
        "          leg.AddEntry(h, glabel,\"l\")\n",
        "   if leg: leg.Draw()\n",
        "   canvas.Update()\n",
        "   canvas.Print(out_file + '.pdf')\n",
        "   if ifC:\n",
        "      canvas.Print(out_file + '.C')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   main()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7 files. \n",
            "Searching in : /content/drive/MyDrive/dataset/cern/Ele_VarAngleMeas*.h5\n",
            "Found 7 files. \n",
            "@@@@@ :  7\n",
            "Loading Data from ..... /content/drive/MyDrive/dataset/cern/Ele_VarAngleMeas_100_200_006.h5\n",
            "X reshape at begining  (5000, 51, 51, 25)\n",
            "X reshape at begining  (5000, 51, 51, 25)\n",
            "******** ROOT DEBUG ******* <HDF5 file \"Ele_VarAngleMeas_100_200_006.h5\" (mode r)>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    500\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n\u001b[0;32m--> 501\u001b[0;31m             graph._c_graph, serialized, options)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopedTFImportGraphDefResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: NodeDef missing attrs 'postscale_factor', 'prescale_factor' from Op<name=HorovodAllreduce; signature=tensor:T -> sum:T; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=reduce_op:int; attr=prescale_factor:float; attr=postscale_factor:float; attr=ignore_name_scope:bool,default=false>; NodeDef: {{node DistributedAdamOptimizer_Allreduce/HorovodAllreduce_gradients_1_generator_generator_in_dense_mul_grad_tuple_control_dependency_0}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c328f18c7da5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m    \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-c328f18c7da5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m    \u001b[0;31m#saver = tf.train.Saver()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'******** ROOT DEBUG *******'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m    \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/code/cern3/model/model_5.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m    \u001b[0;31m#saver = tf.train.import_meta_graph('/home/achaibi/scratch/applications/cern/runs8/runs/pgan/2020-11-10_09:57:44/model_4.meta')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m    \u001b[0;31m#saver = tf.train.import_meta_graph( os.path.join(gweights,'model_4_ckpt_4160000.meta'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[1;32m   1452\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                                  **kwargs)[0]\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m           \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m           \u001b[0mreturn_elements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_elements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m           **kwargs))\n\u001b[0m\u001b[1;32m   1478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m   saver = _create_saver_from_imported_meta_graph(meta_graph_def, import_scope,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mimport_scoped_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mproducer_op_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproducer_op_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         return_elements=return_elements)\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;31m# TensorFlow versions before 1.9 (not inclusive) exported SavedModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    403\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0mop_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m       producer_op_list=producer_op_list)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;31m# Create _DefinedFunctions for any imported functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: NodeDef missing attrs 'postscale_factor', 'prescale_factor' from Op<name=HorovodAllreduce; signature=tensor:T -> sum:T; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=reduce_op:int; attr=prescale_factor:float; attr=postscale_factor:float; attr=ignore_name_scope:bool,default=false>; NodeDef: {{node DistributedAdamOptimizer_Allreduce/HorovodAllreduce_gradients_1_generator_generator_in_dense_mul_grad_tuple_control_dependency_0}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nkq6TxSDj39",
        "outputId": "a75f0603-8c2d-417f-9aa0-2924b5fecb3c"
      },
      "source": [
        "!git push origin adel/pgan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: could not read Password for 'https://{708e5c851440f2d63bc52cbda896495c250f3267}@github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGkc5s0iG_ZK",
        "outputId": "24fb194b-54bd-4739-aecc-226c3d922daf"
      },
      "source": [
        "# Clone github repository setup\n",
        "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
        "from os.path import join  \n",
        "\n",
        "# path to your project on Google Drive\n",
        "MY_GOOGLE_DRIVE_PATH = \"/content/drive/MyDrive/code/\"\n",
        "# replace with your Github username \n",
        "GIT_USERNAME = \"adelc\" \n",
        "# definitely replace with your\n",
        "GIT_TOKEN = \"708e5c851440f2d63bc52cbda896495c250f3267\"  \n",
        "# Replace with your github repository in this case we want \n",
        "# to clone deep-learning-v2-pytorch repository\n",
        "GIT_REPOSITORY = \"3Dgan\" \n",
        "\n",
        "PROJECT_PATH = join(\"/\", MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "# It's good to print out the value if you are not sure \n",
        "print(\"PROJECT_PATH: \", PROJECT_PATH) \n",
        "\n",
        "# In case we haven't created the folder already; we will create a folder in the project path \n",
        "#!mkdir \"cern\"    \n",
        "\n",
        "#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "print(\"GIT_PATH: \", GIT_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROJECT_PATH:  /content/drive/MyDrive/code/\n",
            "GIT_PATH:  https://708e5c851440f2d63bc52cbda896495c250f3267@github.com/adelc/3Dgan.git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eL6eYqlIjG7",
        "outputId": "c98b11b0-e59e-43b4-a0d6-d8de61a3a26b"
      },
      "source": [
        "!git push origin adel/pgan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: could not read Password for 'https://{708e5c851440f2d63bc52cbda896495c250f3267}@github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq-TD2WXp9dD",
        "outputId": "53cca1d6-5477-49a5-8995-7e8a9250cc01"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3Dgan  model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}