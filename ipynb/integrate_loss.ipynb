{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing Anglegan Keras Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AngleGAN\n",
    "### https://github.com/adelC/3Dgan/blob/Anglegan/keras/AngleTrain3dGAN.py\n",
    "    combined = Model(input=[latent],output=[fake, aux, ang, ecal, add_loss],name='combined_model')\n",
    "    combined.compile(optimizer=RMSprop(lr),loss=['binary_crossentropy', 'mean_absolute_percentage_error', 'mae', 'mean_absolute_percentage_error', 'mean_absolute_percentage_error'],loss_weights=loss_weights)\n",
    "    ...\n",
    "    discriminator.evaluate( X, [y, aux_y, ang, ecal, add_loss], verbose=False, batch_size=batch_size)\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\n",
    "        Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.)\n",
    "    ...\n",
    "    train_on_batch() is used to update weights\n",
    "    ...\n",
    "    https://github.com/adelC/3Dgan/blob/Anglegan/keras/AngleTrain3dGAN.py#L400\n",
    "    disc_test_loss.append(discriminator.evaluate( X, [y, aux_y, ang, ecal, add_loss], verbose=False, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Source Code\n",
    "### training.py --> def compile()\n",
    "    defines the loss function, the optimizer and the metrics!\n",
    "    self.compiled.loss = compile_utils.LossesContainer(metrics, weighted_metrics, output_names=self.output_names)\n",
    "### LossesContainer() --> def build():\n",
    "    maybe_broadcast_to_outputs(y_pred, self.losses)\n",
    "        determines if losses/metrics should be applied to all outputs, applies losses to all y_pred --> returns nested structure of objects\n",
    "    conform_to_outputs(y_pred, self_losses)\n",
    "        maps struct to outputs structure\n",
    "    nest.map_structure(self.get_loss_object,self._losses)\n",
    "        apply function to each entry in structure --> returns new structure\n",
    "    nest.flatten(self._losses)\n",
    "        nested structure --> flat list\n",
    "    self._create_metrics()\n",
    "        creates per_output loss metrics (only for multi-output models)\n",
    "    self._built()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to replace keras compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://keras.io/api/losses/\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#Iterate over the batches of a dataset.\n",
    "for x, y in dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass.\n",
    "        logits = model(x)\n",
    "        # Loss value for this batch.\n",
    "        loss_value = loss_fn(y, logits)\n",
    "        # Add extra loss terms to the loss value.\n",
    "        loss_value += sum(model.losses)\n",
    "\n",
    "    # Update the weights of the model to minimize the loss value.\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main2 pgan compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase in range(1, num_phases + 1):\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=d_lr, beta1=args.beta1, beta2=args.beta2)\n",
    "    #...\n",
    "    \n",
    "SEE LOSS.PY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pgan Plan :)\n",
    "    apply loss functions: loss_vector=[binarycrossentropy(fake), mae(ang), meanabsperc(ecal)]\n",
    "    weight and sum losses: dot(loss_vector, weight_vector)\n",
    "    how to integrate with pgan loss functions?!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss_term(optimizer, fake, ang, ecal, loss_weights):\n",
    "    losses = np.array([bce(fake), mae(ang), mape(ecal)])   # calculate the losses and store in an array\n",
    "    # make sure pgan weight vector is a np.array\n",
    "    summed_loss = np.dot(loss_weights, losses)  # weight and sum the losses\n",
    "    return summed_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Code for loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BINARY CROSS ENTROPY\n",
    "#https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/keras/backend.py\n",
    "def binary_crossentropy(target, output, from_logits=False):\n",
    "  \"\"\"Binary crossentropy between an output tensor and a target tensor.\n",
    "  Arguments:\n",
    "      target: A tensor with the same shape as `output`.\n",
    "      output: A tensor.\n",
    "      from_logits: Whether `output` is expected to be a logits tensor.\n",
    "          By default, we consider that `output`\n",
    "          encodes a probability distribution.\n",
    "  Returns:\n",
    "      A tensor.\n",
    "  \"\"\"\n",
    "  # Note: nn.sigmoid_cross_entropy_with_logits\n",
    "  # expects logits, Keras expects probabilities.\n",
    "  if not from_logits:\n",
    "    # transform back to logits\n",
    "    epsilon_ = _to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "    output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\n",
    "    output = math_ops.log(output / (1 - output))\n",
    "  return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n",
    "\n",
    "from tensorflow.python.framework import smart_cond\n",
    "#https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/keras/losses.py#L976-L989\n",
    "def binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):  # pylint: disable=missing-docstring\n",
    "  y_pred = ops.convert_to_tensor(y_pred)\n",
    "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "  label_smoothing = ops.convert_to_tensor(label_smoothing, dtype=K.floatx())\n",
    "\n",
    "  def _smooth_labels():\n",
    "    return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n",
    "\n",
    "  y_true = smart_cond.smart_cond(label_smoothing,\n",
    "                                 _smooth_labels, lambda: y_true)\n",
    "  return K.mean(\n",
    "      K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAE = MEAN ABSOLUTE ERROR\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "  y_pred = ops.convert_to_tensor(y_pred)\n",
    "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "  return K.mean(math_ops.abs(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEAN ABSOLUTE PERCENTAGE ERROR\n",
    "#https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/keras/losses.py#L285-L310\n",
    "class MeanAbsolutePercentageError(LossFunctionWrapper):\n",
    "  \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n",
    "  `loss = 100 * abs(y_true - y_pred) / y_true`\n",
    "  Usage:\n",
    "  ```python\n",
    "  mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "  loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
    "  print('Loss: ', loss.numpy())  # Loss: 5e+08\n",
    "  ```\n",
    "  Usage with the `compile` API:\n",
    "  ```python\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  model.compile('sgd', loss=tf.keras.losses.MeanAbsolutePercentageError())\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               reduction=losses_utils.ReductionV2.AUTO,\n",
    "               name='mean_absolute_percentage_error'):\n",
    "    super(MeanAbsolutePercentageError, self).__init__(\n",
    "        mean_absolute_percentage_error, name=name, reduction=reduction)\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted loss functions -- in progress: apply sigmoid?, implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['binary cross entropy'](1_rdBw0E-My8Gu3f_BOB6GMA.png \"binary cross entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8045189423262791\r\n"
     ]
    }
   ],
   "source": [
    "# BINARY CROSS ENTROPY\n",
    "def bce(output, target, from_logits=False):\n",
    "    output = tf.convert_to_tensor(output)\n",
    "    target = tf.convert_to_tensor(target)\n",
    "    sigmoid_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n",
    "    loss= tf.reduce_mean(sigmoid_loss)\n",
    "    return loss\n",
    "\n",
    "output = np.array([0., 1., 2., 3.])\n",
    "target = np.array([0.1, 1.2, 2.1, 3])\n",
    "tf.print(bce(output, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['MAE'](mean-absolute-error.png \"MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025\r\n"
     ]
    }
   ],
   "source": [
    "# MAE = MEAN ABSOLUTE ERROR\n",
    "def mae(output, target):\n",
    "    output = tf.convert_to_tensor(output)\n",
    "    target = tf.convert_to_tensor(target)\n",
    "    return tf.math.reduce_mean(tf.math.abs(target - output), axis=-1) / len(target)\n",
    "\n",
    "output = np.array([0., 1., 2., 3.])\n",
    "target = np.array([0.1, 1.2, 2.1, 3])\n",
    "tf.print(mae(output, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['MAPE'](dbf9cf7529-2.png \"MAPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.583333333333333\r\n"
     ]
    }
   ],
   "source": [
    "# MAPE = MEAN ABSOLUTE PERCENTAGE ERROR\n",
    "def mape(output, target):\n",
    "    target = tf.convert_to_tensor(target)\n",
    "    output = tf.convert_to_tensor(output)\n",
    "    output_without0 = tf.where(tf.equal(output, 0.), tf.ones_like(output), output)  # to avoid -inf losses when y_true is in denominator\n",
    "    loss = (tf.math.reduce_sum(tf.math.abs(output-target)/output_without0) * 100) / len(target)\n",
    "    return loss\n",
    "\n",
    "output = np.array([0, 1.2, 10., 20., 32.])\n",
    "target = np.array([0, 1., 11., 21., 30.])\n",
    "tf.print(mape(output, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile final loss (weighted and summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output=y_true, target=y_pred=plug model(x)\n",
    "# I need to plug each value into array...what about conditional layers?\n",
    "fake_loss = bce(fake_output, fake_target)\n",
    "ang_loss = mae(ang_output, ang_target)\n",
    "ecal_loss = mape(ecal_output, ecal_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_loss(fake_loss, ang_loss, ecal_loss, loss_weights):\n",
    "    losses = np.array([fake_loss, ang_loss, ecal_loss])   # calculate the losses and store in an array\n",
    "    loss_weights = loss_weights.numpy() # make sure pgan weight vector is a np.array\n",
    "    summed_loss = np.dot(loss_weights, losses)  # weight and sum the losses\n",
    "    return summed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile(optimizer, physics_loss):\n",
    "    # Update the weights of the model to minimize the loss value.\n",
    "    #gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    #optimizer.apply_gradients(zip(gradients, model.trainable_weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
